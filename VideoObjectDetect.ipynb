{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch tf1 into `utils.ops`\n",
    "utils_ops.tf = tf.compat.v1\n",
    "\n",
    "# Patch the location of gfile\n",
    "tf.gfile = tf.io.gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'final_inference_graph'\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "PATH_TO_LABELS = os.path.join('final_training', 'label_map.pbtxt')\n",
    "NUM_CLASSES = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_CKPT , 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_to_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    print(image.size)\n",
    "    return np.array(image.getdata()).reshape((im_width, im_height,3)).astype(np.uint8), im_height, im_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Individual Images\n",
    "PATH_TO_TEST_IMAGES_DIR = \"annotations/test\"    \n",
    "TEST_IMAGE_PATHS = [os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 2)]\n",
    "\n",
    "IMAGE_SIZE = (12,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = cv2.VideoWriter('final_training/Video Files/demo_video_final_1.avi', cv2.VideoWriter_fourcc(\n",
    "        'M', 'J', 'P', 'G'), 10, (1624, 750))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms_tf(bboxes,psocres,threshold):\n",
    "    '''\n",
    "    NMS: NMS using in-built tf.image.non_max_suppression(bboxes,scores,top_n_proposal_after_nms,iou_threshould)\n",
    "        \n",
    "    Input:\n",
    "        bboxes(tensor of bounding proposals) : Bounding Box Proposals in the format (x_min,y_min,x_max,y_max)\n",
    "        threshold(float): Overlapping threshold above which proposals will be discarded.\n",
    "        \n",
    "    Output:\n",
    "        filtered_bboxes(numpy array) :selected bboxes for which IOU is less than threshold.\n",
    "    '''\n",
    "    \n",
    "    #First we need to convert bbox format from (x_min,y_min,x_max,y_max) to (y_min, x_min, y_max, x_max) ..\n",
    "    #because tf.image.non_max_suppression method expects in that form.\n",
    "    #For this we can use tf.unstack and tf.stack\n",
    "    bboxes = tf.cast(bboxes,dtype=tf.float32)\n",
    "    x_min,y_min,x_max,y_max = tf.unstack(bboxes,axis=1)\n",
    "    bboxes = tf.stack([y_min,x_min,y_max,x_max],axis=1)\n",
    "    bbox_indices = tf.image.non_max_suppression(bboxes,psocres, 100, iou_threshold=threshold)\n",
    "    filtered_bboxes = tf.gather(bboxes,bbox_indices)\n",
    "    scores = tf.gather(psocres,bbox_indices)\n",
    "    y_min,x_min,y_max,x_max = tf.unstack(filtered_bboxes,axis=1)\n",
    "    filtered_bboxes = tf.stack([x_min,y_min,x_max,y_max],axis=1)\n",
    "    \n",
    "    \n",
    "    return filtered_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with detection_graph.as_default():\n",
    "    with tf.Session(graph=detection_graph) as sess:\n",
    "        # for image_path in TEST_IMAGE_PATHS:\n",
    "            # image = Image.open(image_path)\n",
    "            # image_np = load_image_to_numpy_array(image)\n",
    "            # image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "#         all_tensor_nodes = [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "#         all_feature_extractors = [x for x in all_tensor_nodes if \"Relu\" in x]\n",
    "#         print(all_tensor_nodes, len(all_tensor_nodes))\n",
    "        image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "        detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "        detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "        num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "#         detection_features = detection_graph.get_tensor_by_name('detection_features:0')\n",
    "        b1s = detection_boxes[0]\n",
    "        s1s = detection_scores[0]\n",
    "        c1s = detection_classes[0]\n",
    "#         df1s = detection_features[0]\n",
    "\n",
    "        selected_indices = tf.image.non_max_suppression(b1s, s1s, 100, iou_threshold = 0.5)\n",
    "\n",
    "        bxs = tf.gather(b1s, selected_indices)\n",
    "        cls = tf.gather(c1s, selected_indices)\n",
    "        scrs = tf.gather(s1s, selected_indices)\n",
    "#         features= tf.gather(df1s, selected_indices)\n",
    "        cap = cv2.VideoCapture('/Users/krunaaltavkar/Downloads/CSCI 527 - Project Data/Test_Video_Files/new_frames_2/video_10%03d.jpg')\n",
    "        while(cap.isOpened()):\n",
    "            # Read the frame\n",
    "            ret, frame = cap.read()\n",
    "            if frame is None:\n",
    "                break\n",
    "            frame = frame.astype('uint8')\n",
    "            # Recolor the frame. By default, OpenCV uses BGR color space.\n",
    "            # This short blog post explains this better:\n",
    "            # https://www.learnopencv.com/why-does-opencv-use-bgr-color-format/\n",
    "            color_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "            image_np_expanded = np.expand_dims(color_frame, axis=0)\n",
    "        \n",
    "            (boxes, scores, classes, num_ds) = sess.run(\n",
    "                [bxs, scrs, cls, num_detections],\n",
    "                feed_dict = {image_tensor: image_np_expanded})\n",
    "            \n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                color_frame,\n",
    "                np.squeeze(boxes),\n",
    "                np.squeeze(classes).astype(np.int32),\n",
    "                np.squeeze(scores),\n",
    "                category_index,\n",
    "                use_normalized_coordinates = True,\n",
    "                line_thickness=8,\n",
    "                min_score_thresh=.40)\n",
    "        \n",
    "            cv2.imshow('frame', color_frame)\n",
    "            output_rgb = cv2.cvtColor(color_frame, cv2.COLOR_RGB2BGR)\n",
    "            out.write(output_rgb)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        out.release()\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
